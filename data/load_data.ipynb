{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c5adda-316b-4dce-85ab-740c0acce8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27894ff7-10eb-488f-a646-afb61a7b096d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/root/autodl-tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60af2ba7-212f-4f7e-be15-8bdebeea7463",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'entities.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentities.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'entities.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'entities.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "861e0a63-f8f6-476d-aa2c-f035e08c3829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    obsolete sudden onset of severe chills\n",
       "1                                         dry hacking cough\n",
       "2                                   pulmonary consolidation\n",
       "3                                                  enanthem\n",
       "4                                                   anxiety\n",
       "                                ...                        \n",
       "814974                               IL-9 signaling pathway\n",
       "814975            Complement activation - classical pathway\n",
       "814976                                    Heme biosynthesis\n",
       "814977    FAS pathway and stress induction of HSP regula...\n",
       "814978          Hepatocyte growth factor receptor signaling\n",
       "Name: name, Length: 814979, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc065178-9fdf-4a5e-9689-57521f60c376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              SYMP:0000149\n",
       "1              SYMP:0000259\n",
       "2              SYMP:0000729\n",
       "3              SYMP:0000746\n",
       "4              SYMP:0000412\n",
       "                ...        \n",
       "814974     WikiPathways:WP8\n",
       "814975    WikiPathways:WP81\n",
       "814976    WikiPathways:WP86\n",
       "814977    WikiPathways:WP89\n",
       "814978    WikiPathways:WP94\n",
       "Name: id, Length: 814979, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6422e788-bef2-49f0-8c7a-b7079a186586",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers_embedder as tre\n",
    "\n",
    "tokenizer = tre.Tokenizer(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "model = tre.TransformersEmbedder(\n",
    "    \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\", subword_pooling_strategy=\"sparse\", layer_pooling_strategy=\"mean\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b9528f7-8547-4068-8926-3b69f18e65cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.1083, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "example = \"hello hello world\"\n",
    "\n",
    "inputs = tokenizer([example, \"hello hello\"], return_tensors=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "embs = outputs.word_embeddings\n",
    "torch.norm(outputs.word_embeddings[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d990428-0ba7-47b4-9102-baf1aefdcb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 3091, 6012, 3091, 6012, 4867,    3],\n",
       "        [   2, 3091, 6012, 3091, 6012,    3,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0]]), 'scatter_offsets': tensor([[ 0,  1,  1,  2,  2,  3,  4],\n",
       "        [ 0,  1,  1,  2,  2,  3, -1]]), 'sentence_lengths': [5, 4], 'sparse_offsets': {'sparse_indices': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 2, 2, 3, 4, 0, 1, 1, 2, 2, 3],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5]]), 'sparse_values': tensor([1.0000, 0.5000, 0.5000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 0.5000,\n",
       "        0.5000, 0.5000, 0.5000, 1.0000]), 'sparse_size': torch.Size([2, 5, 7])}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5759bf9b-3720-4c0a-bd22-5db431ce3f11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 4],\n",
       " tensor([[ 0.1203,  0.2487, -0.1486,  ..., -0.1456,  0.3316, -0.1051],\n",
       "         [-0.0856,  0.3909,  0.0588,  ..., -0.0094, -0.1266, -0.2713],\n",
       "         [ 0.0517,  0.2443,  0.0853,  ...,  0.0598, -0.1438, -0.2997],\n",
       "         [-0.0973, -0.0471, -0.2139,  ..., -0.3524, -0.3174, -0.2248],\n",
       "         [ 0.0008, -0.0177, -0.0147,  ..., -0.0233, -0.0059, -0.0518]],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([[ 0.0747,  0.3440,  0.0568,  ..., -0.3278,  0.4534, -0.0326],\n",
       "         [-0.1360,  0.4439,  0.1482,  ...,  0.0257, -0.1675,  0.0163],\n",
       "         [ 0.0688,  0.3683,  0.1515,  ..., -0.0413, -0.0813,  0.0807],\n",
       "         [ 0.0106,  0.0085, -0.0164,  ...,  0.0052, -0.0130, -0.0304]],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['sentence_lengths'], embs[0], embs[1][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2ad71cd-ddce-450f-a53b-ac65cda33d4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0152e-03,  1.6383e-01, -4.6616e-02,  3.4966e-01,  1.9546e-01,\n",
      "        -5.8523e-02, -1.8699e-01,  7.4290e-02,  2.3773e-01, -1.7750e-01,\n",
      "        -6.6116e-02, -1.9734e-01,  1.9792e-01, -6.5601e-03, -5.3427e-01,\n",
      "        -1.9291e-03,  1.9207e-01, -3.9170e-03, -1.4562e-02,  1.3150e-01,\n",
      "         2.3517e-02,  8.0770e-02,  5.3967e-02, -2.1662e-01,  1.4236e-01,\n",
      "         7.4675e-02, -2.4800e-01, -2.7067e-02, -1.4002e-01,  1.6742e-02,\n",
      "        -3.8091e-01,  1.4589e-01,  2.5638e-01,  1.0902e+00,  1.1738e-01,\n",
      "         3.4021e-01,  3.3545e-02, -1.4539e-01,  1.3752e-01, -3.5649e-01,\n",
      "         1.1106e-01, -2.1805e-01, -1.0710e-01, -4.4380e-02, -2.9830e-01,\n",
      "        -3.9236e-01, -2.9471e-01, -4.2915e-02, -8.7060e-02, -5.3601e-02,\n",
      "        -9.0838e-02,  9.4189e-03, -4.6059e-02, -1.3610e-01, -1.4550e-01,\n",
      "        -1.1841e-01,  8.8128e-02,  1.2702e-01,  1.2624e-01,  2.1803e-01,\n",
      "        -3.3179e-01, -1.3553e-01, -5.7013e-02,  7.2813e-03,  2.6262e-02,\n",
      "        -4.1280e-02, -2.8696e-01, -1.0233e-01,  3.2931e-01,  2.4228e-01,\n",
      "        -5.9157e-02, -4.3708e-02,  3.3666e-01, -2.1755e-01,  2.4408e-01,\n",
      "         2.9044e-01, -9.5275e-02,  2.3226e-01, -9.0755e-02,  5.3158e-01,\n",
      "        -1.6294e-01,  8.9378e-02,  1.4634e-01,  7.8345e-02,  8.0329e-02,\n",
      "        -2.3787e-02, -9.0936e-02, -6.0771e-02, -3.1974e-01, -1.6017e-01,\n",
      "         3.2640e-03, -5.2862e-02,  1.1653e-01,  4.8477e-04, -1.0037e-01,\n",
      "        -1.2099e-02,  3.3270e-01, -7.2463e-02, -1.3447e-01,  8.3992e-02,\n",
      "        -2.6266e-01,  8.8563e-02, -1.4584e-01,  3.9702e-02,  7.3200e-02,\n",
      "        -2.3741e-01, -1.7317e-01, -1.5728e-01, -2.9055e-01, -9.6590e-02,\n",
      "        -3.3731e-01, -2.9214e-01, -1.0393e-01, -3.1345e-01,  1.5981e-01,\n",
      "         1.6168e-02,  6.6400e-02,  4.5111e-02,  4.3817e-01,  5.4068e-02,\n",
      "        -2.0624e-01,  2.7239e-01,  5.2414e-01, -2.1022e-02, -2.4400e-01,\n",
      "        -9.0290e-02,  8.7631e-02, -1.2976e-01, -7.1433e-02,  1.9717e-01,\n",
      "        -2.3765e-01,  5.2715e-01, -1.6571e-01,  9.9806e-02, -2.7485e-02,\n",
      "        -1.4491e-01, -3.1616e-01,  3.5250e-02, -6.4232e-02, -1.0742e-01,\n",
      "         2.2097e-02,  2.0802e-02,  3.9062e-02,  3.0817e-01,  8.1398e-02,\n",
      "         1.0395e-01, -4.6651e-02,  5.9410e-02,  1.7844e-01,  5.9133e-03,\n",
      "        -3.1863e-01,  8.1696e-02, -5.0080e-02,  2.7460e-01, -8.3694e-02,\n",
      "        -1.5949e-01, -1.0936e-01,  1.6529e-01, -8.9619e-02, -2.6642e-01,\n",
      "         2.3788e-01,  2.3921e-02,  5.0105e-02, -1.4693e-01, -1.2593e-01,\n",
      "         3.1736e-02, -1.8941e-01, -2.5852e-01,  1.6733e-01, -2.1683e-02,\n",
      "        -6.6706e-02,  2.6056e-01, -1.2402e-01, -8.9612e-02, -2.4999e-01,\n",
      "         3.9636e-01,  1.0603e-01, -2.0054e-01, -1.2114e-01, -2.9421e-01,\n",
      "         3.1155e-01, -1.6282e+00, -2.4349e-01, -2.1987e-01,  1.6099e-01,\n",
      "         1.3953e-01,  1.1426e-01, -1.9133e-01,  5.5852e-02,  1.0231e-02,\n",
      "         3.0169e-01,  7.2296e-02, -2.5446e-01, -3.3583e-01, -5.3728e-02,\n",
      "         8.3708e-03,  4.2685e-02,  4.6907e-02, -1.2161e-01, -2.2385e-01,\n",
      "         8.1195e-02, -7.4175e-02, -1.9829e-01,  8.8683e-02,  2.6930e-02,\n",
      "         1.4583e-01, -1.0294e-01,  5.6656e-03, -2.9620e-03, -4.7889e-01,\n",
      "        -3.0564e-02, -1.3566e-01, -1.1991e-01, -7.2210e-02, -1.6775e-01,\n",
      "         1.9930e-01, -4.0679e-02,  8.9193e-02, -1.0710e-02, -2.1980e-02,\n",
      "         6.8062e-02, -2.3346e-01,  1.6281e-01, -4.5275e-02,  1.3120e-01,\n",
      "         1.8067e-02, -1.0937e-01,  1.3748e-01,  1.2087e-01,  3.5825e-01,\n",
      "         1.8123e-01,  9.6937e-02,  1.1747e-01, -2.3794e-01, -1.3093e-01,\n",
      "         2.8364e-03, -1.8251e-01,  1.0817e+00, -4.7303e-02, -1.3493e-01,\n",
      "         1.0844e-02,  1.9971e-02, -4.1279e-01, -1.1865e-01, -2.7661e-01,\n",
      "        -2.7001e-01, -1.5963e-01, -1.3286e-02, -4.2622e-02, -2.8688e-01,\n",
      "         6.6315e-02,  4.8485e-02, -2.6541e-01, -7.6484e-02, -5.7509e-01,\n",
      "        -2.7228e-03, -6.3794e-02, -1.3545e-01, -6.5610e-02, -1.2056e-02,\n",
      "         4.3453e-02, -1.0095e-01, -4.0234e-01,  5.8202e-02, -1.4886e-01,\n",
      "         1.0692e-01,  1.7412e-01,  1.3317e-01, -1.2678e-02,  7.7265e-02,\n",
      "        -1.0077e-01,  1.7742e-01,  1.3715e-01,  2.4937e-01,  1.0137e-02,\n",
      "        -1.3111e-01, -3.8197e-02, -4.3818e-01,  3.1037e-01, -1.5088e-01,\n",
      "         2.4205e-01,  4.3788e-01, -1.3314e-01, -3.2543e-01,  3.1932e-01,\n",
      "        -4.2234e-02, -1.0768e-01,  3.2974e-02,  2.7632e-01,  1.0594e-01,\n",
      "         1.7545e-02, -2.2766e-01,  1.9561e-01, -1.8181e-01, -3.6747e-02,\n",
      "        -2.0983e-03,  2.4180e-01,  4.3985e-01,  7.8156e-02, -3.4396e-01,\n",
      "        -2.2536e-01,  1.3200e-01,  1.2078e-02,  7.6081e-02, -9.0546e-02,\n",
      "        -6.4230e-02, -1.7440e-02, -3.4732e-01,  1.2192e-01, -5.0445e-02,\n",
      "        -1.5771e-01,  1.3472e-01,  1.1441e-01, -8.6092e-03, -3.5918e-01,\n",
      "        -2.2841e-01,  2.4945e-02,  1.9065e-01,  1.7803e-01, -3.4515e-01,\n",
      "        -1.7885e-01,  2.7796e-02,  1.2160e-01, -1.5965e-02, -6.0916e-02,\n",
      "         2.1181e-01, -1.4221e-01,  7.0960e-02,  8.6487e-02, -1.7642e-02,\n",
      "        -1.1358e-01,  4.6262e-01,  4.2366e-02,  8.8821e-03,  2.7355e-01,\n",
      "        -7.3036e-02,  3.3850e-01, -1.8884e-01, -2.6247e-02,  4.7270e-01,\n",
      "         1.8090e-02, -9.1797e-04, -1.6787e-01, -4.5485e-03,  1.0475e-01,\n",
      "        -6.6577e-02,  8.9452e-02, -4.0280e-01, -1.7943e-01, -1.4456e-01,\n",
      "        -1.3796e-01, -6.0764e-01, -2.7217e-01, -2.5097e-01, -7.7527e-01,\n",
      "        -4.2538e-02,  2.7518e-01,  4.5509e-01,  2.3099e-01, -8.5411e-02,\n",
      "        -1.9461e-02, -1.0883e-01,  1.8091e-01, -5.7542e-02,  5.2651e-01,\n",
      "         2.3416e-01,  4.5164e-02, -6.7329e-03, -1.8897e-03, -1.5529e-01,\n",
      "        -1.0513e-01,  2.5977e-01, -8.5765e-02,  1.3139e-01,  6.9908e-01,\n",
      "         1.1999e-01, -1.5100e-01, -3.2438e-01, -1.6247e-01, -9.1437e-03,\n",
      "        -2.9833e-01, -9.3417e-02, -3.9191e-01,  1.1263e-01,  1.0996e-01,\n",
      "        -1.5748e-01,  2.8774e-02,  3.8834e-03, -3.1190e-02, -2.2045e-02,\n",
      "        -1.3376e-01, -7.6183e-02,  2.9565e-02,  2.9698e-01, -3.4945e-01,\n",
      "        -8.7794e-02, -1.6744e-01, -3.3249e-01, -6.5817e-02,  1.4115e-01,\n",
      "         8.3286e-02,  2.5044e-01, -5.3774e-02, -7.8417e-02,  3.6691e-01,\n",
      "        -1.7003e-01,  4.9206e-02, -1.9223e-01, -1.7892e-01,  1.1531e-02,\n",
      "         1.9896e-01,  2.4521e-01,  2.4727e-03,  2.1647e-02, -2.3499e-01,\n",
      "         2.9830e-02,  2.9378e-01,  1.2250e-01,  2.6934e-02, -4.9130e-02,\n",
      "        -6.0098e-02,  6.5847e-01,  1.9042e-01, -1.3714e-01, -1.4282e+01,\n",
      "         9.3227e-02, -1.7204e-01,  4.6994e-02,  3.2261e-01, -2.2789e-01,\n",
      "         2.1119e-01,  9.0011e-02,  1.3135e-01, -1.2666e-01, -9.7988e-02,\n",
      "         1.3282e-01, -1.6957e-01,  4.8200e-02, -1.6714e-01,  1.3645e-01,\n",
      "         3.8867e-02, -3.9943e-02,  1.4905e-01, -7.1818e-02,  1.0138e-01,\n",
      "        -8.5218e-02, -1.1243e-01, -5.5417e-01, -1.9533e-01, -2.1525e-01,\n",
      "        -2.4786e-01, -2.5212e-01,  1.8184e-01,  3.4349e-01, -4.3369e-03,\n",
      "        -4.9030e-01, -4.5158e-02,  1.2570e-01, -2.2765e-01, -4.5834e-02,\n",
      "         3.9848e-02,  2.2943e-01,  3.6940e-03, -2.2123e-01, -1.5306e-01,\n",
      "         6.1669e-02,  1.9134e-01, -1.1605e-01, -1.2726e-01,  3.0602e-01,\n",
      "        -6.4532e-02, -9.8704e-02, -3.1173e-02,  1.0939e-01,  4.2521e-02,\n",
      "        -1.1774e-01,  1.0803e-01, -2.2739e-01,  2.9405e-01,  1.0054e-02,\n",
      "         3.8637e-02,  2.7496e-01, -1.8010e-01,  6.2733e-02,  1.4122e-01,\n",
      "        -1.2617e-01, -2.2288e-01,  7.5740e-02, -1.5286e-01, -3.9970e-01,\n",
      "         1.5842e-01, -2.9107e-02,  1.6258e-01, -2.5372e-01, -1.7475e-01,\n",
      "        -1.6757e-02, -2.4639e-02,  4.6181e-02,  6.5363e-01, -4.7519e-02,\n",
      "        -2.1787e-01,  4.3224e-02,  2.8318e+00,  1.4375e-01,  1.0904e-01,\n",
      "         2.6250e-02,  1.4504e-01, -2.5745e-02,  6.0039e-02, -1.5668e-01,\n",
      "         1.6193e-01, -3.2404e-02, -4.0833e-01,  1.7080e-01, -4.9185e-02,\n",
      "         3.4184e-01,  2.5314e-01,  1.2368e-01, -1.9676e-01,  1.1370e-02,\n",
      "         3.9934e-02,  6.4844e-02, -1.8580e-01,  2.1775e-01, -6.2281e-02,\n",
      "         3.8541e-02, -1.2475e-02,  3.5994e-02,  1.4912e-01,  5.3010e-01,\n",
      "        -4.8638e-02,  1.7123e-02, -3.5268e-02,  1.9918e-01, -4.1591e-02,\n",
      "         2.8503e-01,  1.9673e-01, -7.7494e-02, -2.0933e-02,  9.5947e-03,\n",
      "         1.6359e-01,  1.3304e-01, -7.6099e-02, -2.4515e-01,  7.9504e-02,\n",
      "         7.0405e-02, -9.0896e-02,  7.5863e-02,  1.6713e-01,  1.2425e-01,\n",
      "        -1.5148e-01, -2.8314e-01, -2.2258e-02,  1.6678e-01,  3.7962e-01,\n",
      "         1.2038e-02, -4.2334e-01, -2.2017e-01,  1.3126e-01,  5.1422e-02,\n",
      "         3.1035e-01, -5.1666e-02,  2.1864e-01, -1.1981e-01, -5.0594e-02,\n",
      "         2.5390e-02, -7.1584e-03,  1.7857e-01, -1.7744e-01,  1.9885e-01,\n",
      "         1.2326e-01, -1.7971e-01,  7.8233e-03,  1.9677e-01,  8.1289e-02,\n",
      "         6.1754e-02, -9.2896e-02, -5.2510e-02, -1.4994e-02,  6.6038e-02,\n",
      "         2.9883e-02, -6.0429e-02, -2.0792e-01, -3.6437e-01, -1.9009e-01,\n",
      "         2.4108e-01,  4.2755e-02, -1.0215e-01,  2.0856e-01,  7.1416e-02,\n",
      "        -5.2221e-02,  1.8934e-01,  2.3745e-01, -1.1158e-01,  1.6749e-01,\n",
      "         2.3047e-01,  2.7985e-01, -1.5241e-01, -6.9797e-02,  9.4927e-02,\n",
      "         3.6227e-01,  2.7417e-01, -1.7780e-01, -1.9007e-01, -1.5780e-01,\n",
      "         6.6585e-02, -3.8063e-01, -2.5568e-01,  1.0506e-01, -9.3254e-02,\n",
      "         9.9537e-03,  3.7221e-02,  4.7803e-02, -1.2982e-01, -1.7440e-01,\n",
      "         4.2385e-02, -5.4979e-02, -8.7018e-02, -1.4515e-01, -2.8566e-01,\n",
      "         8.1537e-02, -1.6945e-01,  1.4609e-02, -1.5114e-01,  2.1816e-01,\n",
      "        -9.7569e-02, -8.8922e-02,  2.1451e-01, -2.3982e-02, -3.4555e-01,\n",
      "        -1.6119e-01,  7.3233e-01,  6.6542e-02,  8.8415e-03, -3.7289e-01,\n",
      "         1.7438e-01, -3.2531e-02, -1.6214e-01,  2.6872e-01, -3.0507e-01,\n",
      "        -2.4041e-01, -5.3640e-01, -2.9734e-01,  2.9204e-01, -1.3448e-01,\n",
      "         1.8047e-01, -7.9694e-02, -4.3768e-02,  4.9193e-02,  2.0693e-01,\n",
      "        -4.4713e-03, -2.3521e-01,  3.5944e-01, -6.8134e-02,  1.5224e-01,\n",
      "        -2.8223e-01, -1.8420e-01,  2.7060e-01, -2.0455e-02,  1.3773e-01,\n",
      "         4.4700e-02, -2.4869e-01,  1.2560e-01,  1.1293e-01, -9.5296e-02,\n",
      "        -5.6591e-02, -3.2571e-01, -2.7116e-02,  9.7213e-02,  1.9704e-02,\n",
      "         1.2180e-02, -4.0725e-01,  1.1017e-01, -6.7686e-02,  2.2269e-01,\n",
      "        -4.3870e-01,  6.9523e-02, -3.5183e-01, -2.0593e-02, -1.7397e-01,\n",
      "         1.6566e-01,  2.3656e-01,  1.4078e-02, -1.6571e-01,  1.2428e-01,\n",
      "         5.0257e-03,  2.7109e-02, -2.0650e-01,  1.0100e-01,  3.7807e-02,\n",
      "        -4.0930e-02, -8.8660e-02,  1.8868e-01, -9.5702e-02,  4.6412e-02,\n",
      "        -1.8995e-03,  5.7136e-02,  3.4183e-02,  2.3729e-02, -1.9676e-01,\n",
      "         2.8102e-01,  2.2302e-01,  4.1068e-01, -8.5997e-02, -5.4946e-02,\n",
      "        -1.6599e-01, -8.3209e-03, -3.5060e-01, -2.1911e-01,  3.5161e-01,\n",
      "         1.4301e-01, -1.3043e-01, -9.3470e-01,  3.4418e-02, -1.4080e-01,\n",
      "         2.3611e-01,  2.5669e-02, -1.2524e-01, -2.9690e-01, -5.8282e-02,\n",
      "        -3.2619e-02,  2.0769e-01, -3.1039e-02, -2.9462e-02, -1.6136e-01,\n",
      "        -2.1257e-02,  7.8545e-03, -6.2614e-02,  6.0148e-01,  1.3458e-01,\n",
      "         7.8779e-02,  6.4338e-02,  2.9570e-01, -8.5562e-03, -6.8466e-02,\n",
      "         1.3877e-01,  1.8585e-02, -1.5625e-01, -2.1747e-01,  8.7801e-02,\n",
      "        -1.7387e-01, -2.4047e-03,  3.4571e-01, -5.3948e-02,  9.5431e-03,\n",
      "        -9.6407e-02, -2.2616e-01, -1.2790e-01,  9.3733e-02,  7.5109e-02,\n",
      "        -7.1914e-02, -1.9597e-01, -1.6531e-01, -6.5000e-03, -3.2886e-02,\n",
      "         2.8895e-01,  2.5550e-01, -1.7793e-02, -5.8139e-03, -1.3471e-02,\n",
      "         3.8255e-01,  1.8444e-01,  1.9775e-01, -2.7007e-02,  4.2859e-01,\n",
      "        -9.4203e-02, -5.2423e-02, -1.9051e-01], grad_fn=<DivBackward0>)\n",
      "tensor([ 4.5191e-03,  2.9119e-01,  8.5023e-02,  1.7787e-01,  1.0383e-03,\n",
      "         5.9128e-03, -1.1189e-01, -4.3166e-02,  1.5895e-01, -1.8609e-01,\n",
      "        -2.1031e-03, -4.5435e-01,  1.6677e-01,  1.8550e-01, -3.8136e-01,\n",
      "        -2.0588e-03,  1.0659e-01,  2.0089e-01, -7.5164e-02,  1.6406e-01,\n",
      "         5.6662e-02,  1.5625e-01, -3.5507e-02, -2.6034e-01,  9.1789e-02,\n",
      "         3.2664e-01, -2.3268e-01, -1.4028e-01, -7.4818e-02,  1.0201e-01,\n",
      "        -2.9779e-01,  1.2145e-01,  4.0027e-01,  1.1931e+00,  1.5676e-01,\n",
      "         2.4554e-01,  7.4589e-02, -1.0355e-01, -7.0177e-02, -6.2574e-01,\n",
      "         1.8615e-01, -9.0532e-02, -5.0964e-02,  9.7779e-02, -3.6264e-01,\n",
      "        -2.9518e-01, -2.3289e-01,  1.5007e-01, -2.1590e-02, -8.5751e-02,\n",
      "        -7.9303e-02,  5.5332e-03,  2.1468e-01, -1.6689e-01, -1.9968e-01,\n",
      "        -2.0059e-01,  1.9182e-01,  3.1639e-02,  9.2231e-02,  1.4981e-01,\n",
      "        -2.8164e-01, -1.6217e-01, -1.4503e-01, -5.9855e-02, -2.3844e-02,\n",
      "        -6.3759e-02, -2.3967e-02, -1.2224e-01,  3.5109e-01, -5.9322e-02,\n",
      "        -1.5243e-01, -1.5908e-01,  3.2757e-01, -4.1913e-01,  4.5792e-01,\n",
      "         2.4142e-02,  4.4761e-02,  8.4893e-02, -1.5089e-02,  3.3561e-01,\n",
      "        -2.5653e-01, -7.8699e-02,  1.5419e-01,  1.4053e-01, -8.7972e-02,\n",
      "        -1.2794e-01,  1.6358e-02, -7.3697e-02, -3.8956e-01, -1.4938e-01,\n",
      "         2.7607e-02, -1.3378e-01, -7.4336e-02,  7.5256e-02, -9.1531e-02,\n",
      "        -1.2816e-02,  2.3400e-01, -1.7939e-02, -1.2360e-02,  1.2566e-01,\n",
      "        -3.1782e-01,  9.4630e-02, -1.3159e-01, -1.3349e-01,  1.0179e-01,\n",
      "        -1.4201e-01, -6.4128e-02, -4.5386e-03, -2.7493e-01, -1.1994e-01,\n",
      "        -4.7426e-01, -1.6365e-01, -2.0091e-01, -3.6068e-01,  7.3582e-02,\n",
      "         1.3698e-02,  1.1751e-02, -1.8413e-02,  3.6816e-01, -7.3816e-02,\n",
      "        -2.3132e-01,  2.5756e-01,  3.6250e-01,  6.7693e-02, -3.0970e-01,\n",
      "        -3.9916e-02,  6.2087e-02,  1.5081e-02, -1.7702e-01,  3.7520e-01,\n",
      "        -1.5544e-01,  3.6579e-01, -1.6932e-01, -5.8997e-02, -7.4694e-02,\n",
      "        -2.5535e-01, -2.2681e-01, -1.7227e-02,  3.2013e-02, -1.4598e-01,\n",
      "        -1.3091e-03, -9.8202e-02, -8.2985e-02,  2.6745e-02,  2.5345e-02,\n",
      "         7.3933e-02, -6.7080e-02, -8.9816e-03,  2.7720e-01,  1.1811e-01,\n",
      "        -3.7046e-01,  6.2069e-02,  1.8398e-01,  1.7931e-01, -7.3214e-02,\n",
      "        -1.9282e-01, -7.5217e-02, -3.3233e-02, -6.5829e-02, -1.4620e-01,\n",
      "         1.7004e-01,  1.2543e-01,  8.5207e-02, -3.1682e-02, -2.4243e-01,\n",
      "         2.5135e-03, -2.3878e-01, -3.0354e-01,  2.1367e-01, -9.0676e-02,\n",
      "         7.3813e-04,  4.2645e-01, -1.0288e-01,  4.7762e-02, -9.4628e-02,\n",
      "         3.7333e-01,  2.8228e-01, -2.2023e-01, -3.3178e-02, -1.5650e-01,\n",
      "         2.1700e-01, -1.8482e+00, -4.0993e-02,  7.2248e-03,  1.6091e-01,\n",
      "         3.2903e-02, -8.6332e-03, -3.0083e-01, -1.3900e-01, -1.4552e-01,\n",
      "         3.5188e-01,  1.4153e-01, -1.1171e-01, -3.0118e-01, -2.1178e-01,\n",
      "         1.0602e-01,  2.1038e-01,  1.1832e-01,  8.8000e-02, -3.3434e-01,\n",
      "        -1.4383e-02,  1.0747e-01, -2.8611e-01,  4.0434e-04,  1.2813e-01,\n",
      "         2.7797e-01,  4.9939e-02,  1.9793e-01, -1.8517e-01, -5.6350e-01,\n",
      "        -1.4626e-02, -5.0850e-02, -3.1408e-01, -3.4643e-02, -2.4009e-01,\n",
      "         1.0236e-01,  3.9172e-02,  5.7686e-02, -6.3181e-02,  3.7186e-03,\n",
      "         9.7225e-02, -3.2398e-01,  1.2699e-01, -1.2774e-01, -4.4986e-02,\n",
      "        -1.0649e-04, -7.4083e-02,  1.1885e-01,  1.3211e-01,  2.9528e-01,\n",
      "         1.9010e-01, -1.5012e-01,  1.9461e-01,  9.9103e-02,  6.9640e-03,\n",
      "         1.7041e-01, -1.0743e-01,  7.8852e-01, -7.2779e-03, -1.8888e-01,\n",
      "         2.5172e-01, -4.3838e-02, -2.0586e-01, -3.8566e-02, -1.6528e-01,\n",
      "        -1.6837e-01, -1.6878e-01,  3.0041e-02, -9.0920e-02, -2.3147e-01,\n",
      "         1.7954e-01,  9.5538e-02, -2.0722e-01,  1.0685e-01, -4.8787e-01,\n",
      "         1.6322e-01, -1.5007e-01,  9.8717e-02,  1.8544e-02, -1.4503e-01,\n",
      "         6.6525e-02,  1.4119e-01, -3.9922e-01, -1.8552e-01, -4.0168e-01,\n",
      "         6.5898e-02,  9.6222e-02,  3.1253e-01, -1.1475e-02,  1.3772e-01,\n",
      "         8.3677e-02,  2.0820e-01, -5.4140e-02,  1.9821e-01, -1.3621e-01,\n",
      "        -3.8420e-01, -1.6277e-01, -5.6545e-02,  4.5241e-01,  8.2449e-02,\n",
      "         2.4425e-01,  2.6975e-01, -1.8955e-01, -3.2288e-01,  1.0796e-01,\n",
      "         1.0286e-01, -3.6681e-02,  8.5696e-03,  3.6942e-01,  3.2737e-02,\n",
      "         3.9098e-02, -1.0028e-01,  2.2496e-01, -2.6115e-01,  1.0521e-01,\n",
      "         6.3444e-02,  2.2455e-01,  3.4706e-01,  2.3666e-02, -2.8908e-01,\n",
      "        -2.2413e-01,  1.7834e-01, -3.3843e-02,  1.4610e-01,  3.7496e-02,\n",
      "         4.6689e-02,  1.1330e-01, -7.4661e-01,  1.9141e-01,  3.6919e-02,\n",
      "        -1.8167e-01,  2.2095e-02,  2.9020e-02,  7.4207e-02, -2.3178e-01,\n",
      "        -3.0734e-01,  1.0572e-02,  6.0200e-02, -2.0290e-02, -1.8546e-01,\n",
      "        -4.1561e-02, -3.2480e-02,  1.9648e-01,  2.1182e-02,  2.5014e-02,\n",
      "        -4.2484e-03,  1.0852e-01,  2.1518e-01,  5.5852e-02, -6.1512e-02,\n",
      "        -1.7853e-01,  4.5532e-01, -1.4927e-02, -1.3650e-01,  3.4094e-01,\n",
      "         3.2783e-02,  3.5691e-01, -3.2482e-02, -1.5311e-01,  5.2653e-01,\n",
      "         4.3524e-02, -4.7719e-02, -3.3560e-01, -1.0544e-01,  4.8435e-02,\n",
      "        -1.6103e-01,  9.1963e-02, -4.2280e-01, -1.7876e-01, -2.4434e-01,\n",
      "         5.5387e-02, -5.0909e-01, -2.8851e-01, -4.3590e-01, -8.1490e-01,\n",
      "        -1.9817e-01,  1.7415e-01,  4.5136e-01,  5.3471e-02, -8.5903e-02,\n",
      "        -2.0313e-01, -1.8245e-01,  1.8826e-01,  7.0726e-02,  4.6831e-01,\n",
      "         3.5675e-01, -1.0267e-02,  8.5819e-03,  2.7197e-02, -3.0255e-01,\n",
      "        -7.7787e-02,  2.1398e-01, -1.5751e-01, -5.3296e-02,  6.8205e-01,\n",
      "         1.1573e-01, -1.4611e-01,  5.3949e-02, -5.8490e-02, -8.7110e-02,\n",
      "        -2.4717e-01, -1.9772e-01, -3.9163e-01,  2.0313e-01,  2.4926e-01,\n",
      "        -1.0882e-01, -9.6684e-02,  1.4944e-01,  8.7010e-02,  4.1027e-04,\n",
      "        -1.8233e-01, -8.1847e-02, -1.0919e-02,  2.2067e-01, -1.8756e-01,\n",
      "        -1.8937e-01,  7.9046e-03, -2.7098e-01, -1.3484e-01,  1.0395e-02,\n",
      "        -1.0584e-02,  3.5851e-01, -1.7022e-02,  1.1089e-02,  2.7273e-01,\n",
      "        -2.8723e-03,  1.0473e-01,  5.7607e-02,  1.4408e-04,  1.0158e-02,\n",
      "         1.3171e-02,  4.5593e-01,  4.3411e-02,  1.4991e-02, -4.1228e-01,\n",
      "        -1.1519e-01,  2.3997e-01,  2.2028e-01, -5.9103e-02, -1.5199e-01,\n",
      "        -8.7043e-02,  4.7785e-01,  2.4641e-01,  7.9240e-02, -1.3731e+01,\n",
      "         1.6156e-01, -9.5988e-02, -6.4139e-02,  3.1859e-01, -1.9900e-01,\n",
      "         5.5352e-02,  2.1161e-01,  1.5666e-01,  4.9698e-02,  6.0439e-04,\n",
      "        -6.0506e-02, -1.5237e-01, -3.9819e-02, -1.7200e-01,  2.9657e-01,\n",
      "         6.9159e-03, -7.6040e-02, -1.7513e-01, -2.0940e-02,  2.6904e-01,\n",
      "        -1.9352e-01, -1.9468e-01, -3.0036e-01, -1.2716e-01, -1.6820e-01,\n",
      "        -3.5880e-01, -2.5895e-01,  4.8877e-04,  2.2413e-01,  5.4075e-03,\n",
      "        -4.1399e-01, -1.0010e-01,  1.4992e-01, -2.7238e-01, -1.7199e-01,\n",
      "        -6.7542e-03,  6.1099e-02,  2.1332e-01, -9.8458e-02, -2.2367e-01,\n",
      "         8.6563e-02,  1.4097e-01, -1.1918e-01, -7.7234e-02,  2.8492e-01,\n",
      "         5.8610e-02, -2.0855e-01, -2.2899e-02,  1.0581e-01, -3.9163e-03,\n",
      "        -8.4868e-02,  6.7590e-04, -2.0166e-01,  3.6153e-01, -8.5757e-02,\n",
      "         7.7636e-02,  1.7911e-01, -2.2988e-01,  1.2226e-01,  1.4849e-01,\n",
      "        -3.7678e-01, -7.5576e-02,  2.8311e-02, -1.9336e-01, -6.2517e-01,\n",
      "         1.6126e-01, -2.6612e-02, -3.6164e-03, -3.6498e-01, -1.7082e-01,\n",
      "         2.2575e-02,  3.3498e-01,  7.4021e-02,  8.3917e-01,  1.4095e-02,\n",
      "        -2.7844e-01, -2.9395e-02,  2.6153e+00,  4.6520e-02,  2.0501e-01,\n",
      "         2.1609e-02, -6.1928e-02, -1.4258e-01,  7.7866e-02, -3.7721e-01,\n",
      "         3.0933e-01, -1.6001e-01, -4.5099e-01,  2.3338e-01, -2.4002e-01,\n",
      "         1.2143e-01,  3.2206e-01,  2.4409e-01, -2.0649e-01, -7.6072e-02,\n",
      "         1.3099e-01, -1.3836e-01, -2.6525e-01,  2.7083e-01, -1.1641e-01,\n",
      "         1.2525e-01, -1.9152e-01,  2.9996e-02,  5.4655e-02,  7.0478e-01,\n",
      "        -9.9858e-02, -1.2027e-01,  9.2662e-02,  7.1240e-02,  2.8873e-02,\n",
      "         1.3788e-01,  2.1436e-01, -1.6343e-02, -2.6870e-02,  4.2777e-02,\n",
      "         1.7485e-01,  2.0018e-01, -7.6200e-02, -2.5315e-01,  1.5344e-04,\n",
      "         2.7030e-01, -2.3398e-01, -9.5420e-02,  1.0578e-02,  1.1460e-01,\n",
      "        -1.7936e-01, -2.7265e-01, -1.1521e-01,  4.5646e-01,  2.5135e-01,\n",
      "         1.4121e-01, -3.8381e-01, -8.7419e-02,  1.4224e-02,  1.4717e-01,\n",
      "         3.9365e-01, -1.8765e-01,  3.9975e-01, -1.5428e-01, -2.6268e-02,\n",
      "         4.1595e-02, -1.5448e-01,  1.3846e-01, -3.6275e-01,  9.2590e-02,\n",
      "        -1.6492e-01, -1.0069e-01, -4.6025e-02,  9.2167e-02,  8.1525e-02,\n",
      "        -2.4205e-02,  5.0804e-02, -2.0099e-01, -2.0668e-01, -2.1254e-01,\n",
      "         1.1787e-03,  1.0119e-01, -1.6279e-01, -3.1753e-01, -2.7151e-01,\n",
      "         3.3142e-02,  6.5866e-02,  1.9659e-02,  3.0146e-01,  4.1418e-03,\n",
      "         5.1928e-02,  2.9137e-01,  4.1704e-01, -6.2754e-02,  2.4013e-01,\n",
      "         3.6593e-01,  8.2338e-02, -2.5896e-01,  3.5121e-02, -1.8098e-02,\n",
      "         3.8019e-01,  3.0315e-01, -1.0109e-01, -3.9521e-02, -2.7672e-01,\n",
      "         9.6947e-02, -3.8163e-01, -2.4748e-02,  1.1427e-01, -1.2054e-01,\n",
      "         6.4860e-02,  1.5690e-02,  1.2670e-01, -7.8931e-02, -1.3506e-01,\n",
      "         5.2005e-02, -1.1166e-01,  6.9783e-02,  1.9693e-02, -3.5090e-01,\n",
      "         1.2800e-01, -1.0779e-01,  1.6626e-02, -1.6370e-01,  3.9673e-01,\n",
      "        -3.2325e-01, -1.1158e-01,  2.9118e-01,  6.0123e-05, -3.4290e-01,\n",
      "         2.1074e-01,  5.7529e-01,  8.7516e-02,  6.1953e-02, -4.8240e-01,\n",
      "         1.4692e-01,  1.0985e-01,  1.5600e-01,  2.6350e-01, -2.1355e-01,\n",
      "        -3.6238e-01, -4.9087e-01, -2.7460e-01,  1.6739e-01, -5.6189e-02,\n",
      "         1.6607e-01,  6.8791e-02, -9.9620e-02,  4.3558e-02,  1.4583e-01,\n",
      "        -4.6821e-02, -2.4285e-01,  3.1978e-01, -4.8771e-02, -1.1533e-01,\n",
      "        -2.6881e-01, -2.4742e-01,  1.8652e-01,  1.5184e-01,  2.2575e-01,\n",
      "         1.0919e-01, -1.9969e-01,  8.7234e-02,  2.0087e-01,  3.4986e-02,\n",
      "         2.5068e-01, -1.5366e-01, -1.4230e-02,  7.3332e-02,  1.1306e-01,\n",
      "         4.9478e-03, -6.2247e-01,  2.8428e-01,  1.4717e-02, -2.5323e-02,\n",
      "        -2.4343e-01, -6.4856e-02, -2.5864e-01, -2.4792e-02, -2.2014e-01,\n",
      "         1.7536e-01,  1.7204e-01,  2.1458e-01, -1.9559e-01,  1.5058e-01,\n",
      "        -1.9873e-02,  3.0117e-02, -3.8144e-01,  9.1505e-02,  3.4681e-02,\n",
      "        -1.1127e-01,  1.1378e-01,  1.5848e-01,  2.9541e-01,  4.5241e-02,\n",
      "         6.0014e-02,  1.3264e-01,  2.3164e-01,  6.9426e-02, -3.2072e-01,\n",
      "         3.3719e-01,  2.7263e-01,  4.3660e-01, -8.9983e-02, -2.9314e-01,\n",
      "        -6.3384e-02,  1.0780e-01, -3.7022e-01, -2.8650e-01,  3.8099e-01,\n",
      "         5.1374e-02, -3.5277e-02, -8.8268e-01,  1.2125e-02,  1.1908e-01,\n",
      "         2.7054e-01, -2.0266e-02, -2.2865e-01, -2.2295e-01, -3.5006e-02,\n",
      "         7.0968e-02,  5.6582e-02, -3.1483e-01, -3.3126e-02, -2.1633e-01,\n",
      "        -9.1463e-02,  2.8825e-03, -1.3555e-01,  6.3599e-01,  1.5309e-01,\n",
      "         1.8507e-01,  8.2714e-02,  2.2460e-01, -1.0205e-01, -7.2800e-02,\n",
      "         2.5177e-02, -1.0288e-01, -2.6513e-01, -1.7388e-01,  2.0246e-01,\n",
      "        -1.4951e-01, -4.8587e-02,  1.0569e-01, -1.5140e-01,  8.0532e-02,\n",
      "         1.1764e-01, -4.2465e-01, -1.3588e-01, -1.9031e-01,  1.6973e-01,\n",
      "        -1.5548e-01, -1.2676e-01, -2.2591e-01, -1.0660e-02, -2.6564e-01,\n",
      "         1.8716e-01,  3.0780e-01,  1.2250e-01, -5.4111e-02, -1.1910e-02,\n",
      "         2.3812e-01,  1.6020e-01,  1.6926e-01,  6.8448e-02,  3.0076e-01,\n",
      "        -8.4522e-02,  4.7882e-02,  8.5157e-03], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embs = outputs.word_embeddings\n",
    "for i in range(embs.shape[0]):\n",
    "    embs[i][:inputs['sentence_lengths'][i]].sum(dim=0) / inputs['sentence_lengths'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d796332-e2df-4c74-a3f2-bc148c722f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sent_vecs = embs.sum(dim=1) / embs.count_nonzero(dim=1)\n",
    "sent_vecs_normalized = sent_vecs / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bcfebf4-2ae9-4b7a-ab8c-b24e89cc044c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vecs = np.array(sent_vecs.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdc44906-5209-41f3-aa28-b25abd21fd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "faiss.normalize_L2(vecs)\n",
    "d = vecs.shape[1]\n",
    "index = faiss.IndexFlatIP(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc5f74bb-7e0d-494e-b985-b928a26f3652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index.add(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79b4bcc9-6a44-4769-a299-faf4ad6e710f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as  np\n",
    "np.array([torch.tensor([1,2,3,4]).numpy(), torch.tensor([1,2,3,4]).numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e98f54a0-7c42-4521-83a3-f66f776f156f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = vecs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8493913-50f6-46d1-9c12-9daf24d64f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "D, I = index.search(query[np.newaxis, :], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b6d9138-b02e-4309-806a-d06970da0348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "距离:  [0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999 0.9999999\n",
      " 0.9999999 0.9999999]\n",
      "索引:  [ 1  3  4  5  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52\n",
      " 53 54 55 56 57 58 59 60 61 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77\n",
      " 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 62 30\n",
      " 14  6  2  0]\n"
     ]
    }
   ],
   "source": [
    "print(\"距离: \", D.flatten())\n",
    "print(\"索引: \", I.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeddd87-aa90-4866-bdc8-c7071feaf2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831a2d4c-22bb-4311-896a-59b488f28278",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/tmp/ipykernel_1007/1121646524.py:13: DtypeWarning: Columns (6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "import transformers_embedder as tre\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tokenizer = tre.Tokenizer(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "model = tre.TransformersEmbedder(\n",
    "    \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\", subword_pooling_strategy=\"sparse\", layer_pooling_strategy=\"mean\"\n",
    ")\n",
    "\n",
    "\n",
    "file_path = 'entities.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8051905-e0ee-4b0b-beff-890a6e1b2ead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581176"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = df.loc[:,'name'].to_list()\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcee4b1-7bd6-4fe9-9f05-b2fd03dfcb12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(names[:5000], return_tensors=True, padding=True)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc63278-dec5-452a-aedf-bb4ab79ad421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc6fdbf-abac-4b49-98e2-43abeaf00e91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1007/3637672984.py:4: DtypeWarning: Columns (6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'entities.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "names = df.loc[:,'name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44fd2044-158e-4b3f-83ea-8e2cf478ee49",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lens = [len(name.split()) for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21221b39-1af9-4dac-a906-a1b558d8bd36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example list of integers\n",
    "data = lens[:] # Generating a sample list of integers\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(data, bins=50, alpha=0.75, color='blue')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Integer Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b46e1afb-bbfe-4fe7-b90c-5a382a68e6c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "814979"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ae65fe3-cf8b-4a67-85cb-0501cf2fbd32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350789\n",
      "torch.Size([1000, 7, 768])\n",
      "torch.Size([1000, 11, 768])\n",
      "torch.Size([1000, 11, 768])\n",
      "torch.Size([1000, 106, 768])\n",
      "torch.Size([1000, 122, 768])\n",
      "torch.Size([1000, 42, 768])\n",
      "torch.Size([1000, 36, 768])\n",
      "torch.Size([1000, 56, 768])\n",
      "torch.Size([1000, 53, 768])\n",
      "torch.Size([1000, 39, 768])\n",
      "torch.Size([1000, 7, 768])\n",
      "torch.Size([1000, 54, 768])\n",
      "torch.Size([1000, 49, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(l) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m chunk_len):\n\u001b[1;32m     10\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(l[j\u001b[38;5;241m*\u001b[39mchunk_len:(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mchunk_len], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mword_embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers_embedder/embedder.py:166\u001b[0m, in \u001b[0;36mTransformersEmbedder.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, scatter_offsets, sparse_offsets, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_type_ids\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Shape: [batch_size, num_sub-words, embedding_size].\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_pooling_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    168\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m transformer_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:307\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 307\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    309\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    311\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "chunk_len = 1000\n",
    "\n",
    "for i in range(1, 27+1):\n",
    "    l = [name for j, name in enumerate(names) if len(name.split()) == i]\n",
    "    idx = [j for j, name in enumerate(names) if len(name.split()) == i]\n",
    "    print(len(l))\n",
    "    for j in range(len(l) // chunk_len):\n",
    "        inputs = tokenizer(l[j*chunk_len:(j+1)*chunk_len], return_tensors=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        print(outputs.word_embeddings.shape)\n",
    "        del inputs\n",
    "        del outputs\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "338800c8-85ac-4668-a870-4274dbefc01c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.65 GiB total capacity; 22.70 GiB already allocated; 6.06 MiB free; 23.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m chunk_len):\n\u001b[1;32m     10\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(names[j\u001b[38;5;241m*\u001b[39mchunk_len:(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mchunk_len], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mword_embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers_embedder/embedder.py:166\u001b[0m, in \u001b[0;36mTransformersEmbedder.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, scatter_offsets, sparse_offsets, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_type_ids\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Shape: [batch_size, num_sub-words, embedding_size].\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_pooling_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    168\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m transformer_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:363\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 363\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    366\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.65 GiB total capacity; 22.70 GiB already allocated; 6.06 MiB free; 23.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "chunk_len = 1000\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# tokenizer = tokenizer.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "for j in range(len(names) // chunk_len):\n",
    "    inputs = tokenizer(names[j*chunk_len:(j+1)*chunk_len], return_tensors=True, padding=True).to(device)\n",
    "    outputs = model(**inputs).detach()\n",
    "    print(outputs.word_embeddings.shape)\n",
    "    del inputs\n",
    "    del outputs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5a805c1-66c3-4c14-88c9-42a6f733122f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 6         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  23200 MiB |  23241 MiB | 555638 MiB | 532438 MiB |\n",
      "|       from large pool |  23190 MiB |  23231 MiB | 553713 MiB | 530522 MiB |\n",
      "|       from small pool |      9 MiB |     10 MiB |   1925 MiB |   1915 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  23200 MiB |  23241 MiB | 555638 MiB | 532438 MiB |\n",
      "|       from large pool |  23190 MiB |  23231 MiB | 553713 MiB | 530522 MiB |\n",
      "|       from small pool |      9 MiB |     10 MiB |   1925 MiB |   1915 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  23189 MiB |  23230 MiB | 555260 MiB | 532071 MiB |\n",
      "|       from large pool |  23179 MiB |  23220 MiB | 553335 MiB | 530156 MiB |\n",
      "|       from small pool |      9 MiB |     10 MiB |   1924 MiB |   1914 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  23746 MiB |  23746 MiB |  76318 MiB |  52572 MiB |\n",
      "|       from large pool |  23734 MiB |  23734 MiB |  76288 MiB |  52554 MiB |\n",
      "|       from small pool |     12 MiB |     12 MiB |     30 MiB |     18 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 558598 KiB |   4799 MiB | 236012 MiB | 235466 MiB |\n",
      "|       from large pool | 556397 KiB |   4797 MiB | 233959 MiB | 233416 MiB |\n",
      "|       from small pool |   2201 KiB |      3 MiB |   2052 MiB |   2050 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     426    |     427    |    9518    |    9092    |\n",
      "|       from large pool |     231    |     232    |    6105    |    5874    |\n",
      "|       from small pool |     195    |     196    |    3413    |    3218    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     426    |     427    |    9518    |    9092    |\n",
      "|       from large pool |     231    |     232    |    6105    |    5874    |\n",
      "|       from small pool |     195    |     196    |    3413    |    3218    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     162    |     252    |     561    |     399    |\n",
      "|       from large pool |     156    |     247    |     546    |     390    |\n",
      "|       from small pool |       6    |       6    |      15    |       9    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     140    |     149    |    5913    |    5773    |\n",
      "|       from large pool |     131    |     143    |    4378    |    4247    |\n",
      "|       from small pool |       9    |       9    |    1535    |    1526    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# del tokenizer\n",
    "# del model\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddc3771e-ff61-4248-8487-a6702f1a7713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = list(range(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d681ca0-716d-4a12-9dc2-7618bdd8394a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[10:999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553b9a7-4de8-49e2-b7ac-8082f0226497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
