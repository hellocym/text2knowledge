{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the BioBERT Vocab uncased model as that is what is recommended on the official GitHub Page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioBERT Embeddings Analysis\n",
    "This is a basic tutorial of how to download and use the BioBERT model to create naive embeddings, which can be used for exploring concepts in the literature corpus. Of course long term we would probably want to fine-tune this model in a unsupervised fashion on the document corpus. Additionally, many of the demonstrated techniques are naive (for instance simply averaging the word embeddings to form a sentence embedding), however this demonstrates how embeddings could be used for this challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROXY\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/cym/text2knowledge/text2knowledge\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-output": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.10/site-packages (4.37.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting tensorflow\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e3/ba/aa8a76eff5c20761b0361a5b4c9fccb8742c29a82adba7a8ad8ae819984e/tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fd/70/8a133c88a394394dd57159083b86a564247399440b63f2da0ad727593570/wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3b/d3/ecb4b3d2ec2c84132987e5f12ab1408f455bec1d90cd5bc408ebf37800f5/h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m138.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard<2.16,>=2.15 in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (1.26.3)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fc/a7/0d4490de967a67f68a538cc9cdb259bff971c4b5787f7765dc7c8f118f71/keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=23.5.26\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b6/c8/2f823c8958d5342eafc6dd3e922f0cc4fcf8c2e0460284cc462dae3b60a0/tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (1.60.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7a/5f/2cce4de2189f72e8d0b2bf7de1f3270cdaf397f8458008e79584b024e5a4/tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ea/df/55525e489c43f9dbb6c8ea27d8a567b3dcd18a22f3c45483055f5ca6611d/libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /root/miniconda3/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d1/1d/d5cf76e5e40f69dbd273036e3172ae4a614577cb141673427b80cac948df/ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /root/miniconda3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /root/miniconda3/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /root/miniconda3/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /root/miniconda3/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /root/miniconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /root/miniconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /root/miniconda3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /root/miniconda3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, opt-einsum, ml-dtypes, keras, h5py, google-pasta, gast, astunparse, tensorflow\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-pasta-0.2.0 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 opt-einsum-3.3.0 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 wrapt-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--2024-02-18 22:12:47--  https://github.com/naver/biobert-pretrained/releases/download/v1.1-pubmed/biobert_v1.1_pubmed.tar.gz\n",
      "Connecting to 172.20.0.113:12798... connected.\n",
      "WARNING: cannot verify github.com's certificate, issued by ‘emailAddress=autodl@gmail.com,CN=autodl,OU=autodl,O=AutoDL,L=Lavinia,ST=Westen,C=AU’:\n",
      "  Self-signed certificate encountered.\n",
      "Proxy request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/167883658/353e7a00-7804-11e9-8e2a-b47e8b3e93bc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240218%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240218T141117Z&X-Amz-Expires=300&X-Amz-Signature=3d64a989cb4f02e9051af95cf7312ed71c3ad5f23641febf0a5b84db6ee0e74f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=167883658&response-content-disposition=attachment%3B%20filename%3Dbiobert_v1.1_pubmed.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-02-18 22:12:48--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/167883658/353e7a00-7804-11e9-8e2a-b47e8b3e93bc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240218%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240218T141117Z&X-Amz-Expires=300&X-Amz-Signature=3d64a989cb4f02e9051af95cf7312ed71c3ad5f23641febf0a5b84db6ee0e74f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=167883658&response-content-disposition=attachment%3B%20filename%3Dbiobert_v1.1_pubmed.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Connecting to 172.20.0.113:12798... connected.\n",
      "WARNING: cannot verify objects.githubusercontent.com's certificate, issued by ‘emailAddress=autodl@gmail.com,CN=autodl,OU=autodl,O=AutoDL,L=Lavinia,ST=Westen,C=AU’:\n",
      "  Self-signed certificate encountered.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 401403346 (383M) [application/octet-stream]\n",
      "Saving to: ‘scibert_uncased.tar’\n",
      "\n",
      "scibert_uncased.tar 100%[===================>] 382.81M   351MB/s    in 1.1s    \n",
      "\n",
      "2024-02-18 22:12:50 (351 MB/s) - ‘scibert_uncased.tar’ saved [401403346/401403346]\n",
      "\n",
      "biobert_v1.1_pubmed/\n",
      "biobert_v1.1_pubmed/model.ckpt-1000000.data-00000-of-00001\n",
      "biobert_v1.1_pubmed/model.ckpt-1000000.meta\n",
      "biobert_v1.1_pubmed/bert_config.json\n",
      "biobert_v1.1_pubmed/vocab.txt\n",
      "biobert_v1.1_pubmed/model.ckpt-1000000.index\n",
      "Building PyTorch model from configuration: BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 22:13:00.018220: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 22:13:00.080111: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-18 22:13:00.080145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-18 22:13:00.081972: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-18 22:13:00.092943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 22:13:01.212893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow\n",
    "!wget -O scibert_uncased.tar https://github.com/naver/biobert-pretrained/releases/download/v1.1-pubmed/biobert_v1.1_pubmed.tar.gz --no-check-certificate\n",
    "!tar -xvf scibert_uncased.tar\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n",
    "    # Initialise PyTorch model\n",
    "    config = BertConfig.from_json_file(bert_config_file)\n",
    "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
    "    model = BertForPreTraining(config)\n",
    "\n",
    "    # Load weights from tf checkpoint\n",
    "    load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
    "\n",
    "    # Save pytorch-model\n",
    "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
    "    torch.save(model.state_dict(), pytorch_dump_path)\n",
    "convert_tf_checkpoint_to_pytorch(\"biobert_v1.1_pubmed/model.ckpt-1000000\", \"biobert_v1.1_pubmed/bert_config.json\", \"biobert_v1.1_pubmed/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_config.json\t\t\tmodel.ckpt-1000000.meta\n",
      "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
      "model.ckpt-1000000.index\t\tvocab.txt\n",
      "config.json\t\t\t\tmodel.ckpt-1000000.meta\n",
      "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
      "model.ckpt-1000000.index\t\tvocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "!ls biobert_v1.1_pubmed\n",
    "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
    "!ls biobert_v1.1_pubmed\n",
    "model_version = 'biobert_v1.1_pubmed'\n",
    "do_lower_case = True\n",
    "model = BertModel.from_pretrained(model_version)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting scikit-learn\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bc/b9/6a637668d69de04b7f8b917e837aff282950601f09998a5f6c9f23f6642d/scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.19.5 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.26.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b1/2c/f504e55d98418f2fcf756a56877e6d9a45dd5ed28b3d7c267b300e85ad5b/threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /root/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.4.1.post1 threadpoolctl-3.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def embed_text(text, model):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states\n",
    "\n",
    "def get_similarity(em, em2):\n",
    "    return cosine_similarity(em.detach().numpy(), em2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_batch(texts, model):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 6])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([1, 4])\n",
      "Similarity for Coronavirus and Flu:[[0.7177292]]\n",
      "Similarity for Coronavirus and MERs:[[0.88032645]]\n",
      "Similarity for Coronavirus and Bog:[[0.7712864]]\n"
     ]
    }
   ],
   "source": [
    "coronavirus_em = embed_text(\"Coronavirus\", model).mean(1)\n",
    "# We will use a mean of all word embeddings.\n",
    "mers_em = embed_text(\"Middle East Respiratory Virus\", model).mean(1)\n",
    "flu_em = embed_text(\"Flu\", model).mean(1)\n",
    "dog_em = embed_text(\"Bog\", model).mean(1)\n",
    "\n",
    "print(\"Similarity for Coronavirus and Flu:\" + str(get_similarity(coronavirus_em, flu_em)))\n",
    "print(\"Similarity for Coronavirus and MERs:\" + str(get_similarity(coronavirus_em, mers_em)))\n",
    "print(\"Similarity for Coronavirus and Bog:\" + str(get_similarity(coronavirus_em, dog_em)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8665233]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1 = \"Congenital long QT syndrome (LQTS)\"\n",
    "term2 = \"long QT syndrome\"\n",
    "em1 = embed_text(term1, model).mean(1)\n",
    "em2 = embed_text(term2, model).mean(1)\n",
    "get_similarity(em1, em2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see anecdotally even in the raw embeddings there seems to be at least some correlation between concepts. Note that our embedding method \n",
    "\n",
    "Let's now look at visualizing some of these vectors with U-Map. I'm choosing U-Map here due to the high-dimensionality of the data (768-D) and its ability to scale. However, I will also add some T-SNE visualizations below if I have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn\n",
    "import umap\n",
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "def make_the_embeds(number_files, start_range=0, \n",
    "                    the_path=\"/kaggle/input/CORD-19-research-challenge/2020-03-13/comm_use_subset/comm_use_subset\", data_key=[\"metadata\", \"title\"]):\n",
    "    the_list = os.listdir(the_path)\n",
    "    title_embedding_list = [] \n",
    "    title_list = []\n",
    "    for i in range(start_range, number_files):\n",
    "        file_name = the_list[i]\n",
    "        final_path = os.path.join(the_path, file_name)\n",
    "        with open(final_path) as f:\n",
    "            data = json.load(f)\n",
    "        tensor, title = make_data_embedding(data, data_key)\n",
    "        title_embedding_list.append(tensor)\n",
    "        title_list.append(title)\n",
    "    return torch.cat(title_embedding_list, dim=0), title_list\n",
    "        \n",
    "def make_data_embedding(article_data, data_keys, method=\"mean\", dim=1):\n",
    "    data = article_data\n",
    "    for key in data_keys:\n",
    "        data = data[key]\n",
    "    text = embed_text(data, model)\n",
    "    if method == \"mean\":\n",
    "        return text.mean(dim), data\n",
    "    \n",
    "#embed_list, title_list = make_the_embeds(200)\n",
    "#red = reducer.fit_transform(embed_list.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found 200 to be a good chunk size for running quick analysis as doing a full plot can get kind of crowded and is slow to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.palettes import Spectral10, Category20c\n",
    "from bokeh.palettes import magma\n",
    "import pandas as pd\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(red, title_list, number, color = True):\n",
    "    digits_df = pd.DataFrame(red, columns=('x', 'y'))\n",
    "    digits_df['digit'] = title_list\n",
    "    datasource = ColumnDataSource(digits_df)\n",
    "    plot_figure = figure(\n",
    "    title='UMAP projection of the article title embeddings',\n",
    "    plot_width=890,\n",
    "    plot_height=600,\n",
    "    tools=('pan, wheel_zoom, reset')\n",
    "    )\n",
    "\n",
    "    plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n",
    "    <div>\n",
    "    <div>\n",
    "        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style='font-size: 10px; color: #224499'></span>\n",
    "        <span style='font-size: 10px'>@digit</span>\n",
    "    </div>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    if color:\n",
    "        color_mapping = CategoricalColorMapper(factors=title_list, palette=magma(number))\n",
    "        plot_figure.circle(\n",
    "            'x',\n",
    "            'y',\n",
    "            source=datasource,\n",
    "            color=dict(field='digit', transform=color_mapping),\n",
    "            line_alpha=0.6,\n",
    "            fill_alpha=0.6,\n",
    "            size=7\n",
    "        )\n",
    "        show(plot_figure)\n",
    "    else:\n",
    "        \n",
    "        plot_figure.circle(\n",
    "            'x',\n",
    "            'y',\n",
    "            source=datasource,\n",
    "            color=dict(field='digit'),\n",
    "            line_alpha=0.6,\n",
    "            fill_alpha=0.6,\n",
    "            size=7\n",
    "        )\n",
    "        show(plot_figure)\n",
    "    \n",
    "#make_plot(red, title_list, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do seem to be a few interesing patterns when analyizng with U-Map. However, I believe fine-tuning methods could definitely improve the clustering of groups. Let's examine another chunk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_list2, title_list2 = make_the_embeds(401, 201)\n",
    "#red2 = reducer.fit_transform(embed_list.detach().numpy())\n",
    "#print(len(title_list2))\n",
    "#make_plot(red2, title_list2, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll attempt to make a plot of all ~~9000~~ ~~1000~~ (that did make it run out of RAM)  articles in that directory (warning this might crash your notebook). For fun we'll make these a different 1000 then what we already viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len = len(os.listdir(\"/kaggle/input/CORD-19-research-challenge/2020-03-13/comm_use_subset/comm_use_subset\"))\n",
    "#embed_list, title_list_full = make_the_embeds(2000,1200)\n",
    "#red_full = reducer.fit_transform(embed_list.detach().numpy())\n",
    "#make_plot(red_full, title_list_full, 256, color=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing with T-SNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Search Attempts on Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "search_terms = embed_text(\"coronavirus infection origin\", model).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_closest(search_term_embedding, title_embeddings, original_titles, n=10):\n",
    "    proximity_dict = {}\n",
    "    i = 0 \n",
    "    for title_embedding in title_embeddings:\n",
    "        proximity_dict[original_titles[i]] = {\"score\": get_similarity(title_embedding.unsqueeze(0),search_term_embedding), \n",
    "                                              \"title_embedding\":title_embedding}\n",
    "        i+=1\n",
    "    order_dict = collections.OrderedDict({k: v for k, v in sorted(proximity_dict.items(), key=lambda item: item[1][\"score\"])})\n",
    "    proper_list = list(order_dict.keys())[-n:]\n",
    "    return proper_list, order_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_titles, order_dict = top_n_closest(search_terms, embed_list2, title_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results actually don't seem that bad given the model doesn't have any specific training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\n",
    "import en_core_sci_sm as en\n",
    "\n",
    "\n",
    "nlp = en.load()\n",
    "#while True:\n",
    "#text = input(\"question \")\n",
    "text = \"What is known about covid-19 incubation period?\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(list(doc.ents))\n",
    "txt = \"\"\n",
    "for ent in list(doc.ents):\n",
    "    txt += str(ent)\n",
    "    txt += \" \"\n",
    "\n",
    "search_terms2 = embed_text(txt, model).mean(1)\n",
    "top_titles2, order_dict1 = top_n_closest(search_terms, embed_list2, title_list2)\n",
    "print(top_titles2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Abstracts\n",
    "Just for fun and to enrich our knowledge later let's try embedding abstracts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absd_embeds, abs_orig = make_the_embeds(4, 2, data_key=['abstract', 0, \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the following abstracts will be hard to display in U-Map I won't plot them. Instead let's just look at these two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_orig[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_orig[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity(absd_embeds[0].unsqueeze(0), absd_embeds[1].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly don't know enough about the subject area to tell if that is a good similarity score for those two. I'll add some more examples in a bit, but for now that should serve as good intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 551982,
     "sourceId": 3756201,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29860,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
